# Database Migration Tool Analysis - Comprehensive Guide

**Date:** 2025-01-17 (Updated)  
**Reviewer:** Friday  
**Proposal:** `@openspec/changes/add-sqlite-to-postgres-migration-tool`

---

## Executive Summary

This document provides a comprehensive analysis of database migration strategies to PostgreSQL, covering three primary source scenarios:

1. **SQLite â†’ PostgreSQL** - Legacy database migration
2. **DuckDB â†’ PostgreSQL** - Analytical database migration  
3. **Parquet â†’ PostgreSQL** - Data lake / Python pipeline ingestion

### Key Findings by Source Type

#### SQLite Migration (Original Analysis)
After analyzing the OpenSpec proposal and researching pgloader as an alternative, I recommend a **hybrid approach** that leverages pgloader for data transfer while maintaining a TypeScript wrapper for configuration, validation, and Zero-sync integration. This approach reduces development time by ~70%, eliminates disk space requirements for CSV staging, and provides battle-tested performance.

**Performance:** 30-45 minutes for 101M rows  
**Recommended Tool:** pgloader with TypeScript wrapper

#### DuckDB Migration (New Analysis)
For DuckDB sources, **postgres_scanner extension** (DuckDB-side) provides the fastest and most direct migration path, with Parquet export + parallel COPY as an alternative for very large datasets (100M+ rows). The new **pg_duckdb extension** (PostgreSQL-side) enables hybrid architectures where PostgreSQL can leverage DuckDB's vectorized analytics engine for OLAP queries.

**Performance:** 15-35 minutes for 101M rows  
**Recommended Tools:** 
- Migration: DuckDB postgres_scanner or Parquet export
- Analytics: pg_duckdb for in-place Parquet querying

#### Parquet Pipeline Migration (New Analysis)
When Python data pipelines generate Parquet files (instead of DuckDB databases), **pg_parquet** (CrunchyData) provides native COPY FROM/TO Parquet support with S3/GCS/Azure integration. For maximum performance, **pgpq** (Arrow â†’ PostgreSQL binary COPY streaming) offers the fastest client-side ingestion path.

**Performance:** 15-30 minutes for 101M rows (parallel COPY)  
**Recommended Tools:**
- Server-side: pg_parquet (COPY-based)
- Client-side: pgpq (Arrow streaming)
- Analytics: Parquet FDWs for in-place querying

### Decision Framework

```
Source Data Type?
â”‚
â”œâ”€ SQLite Database
â”‚  â””â”€ Use: pgloader (proven, mature, 30-45 min)
â”‚
â”œâ”€ DuckDB Database
â”‚  â”œâ”€ One-time migration â†’ postgres_scanner (20-35 min)
â”‚  â”œâ”€ Very large (100M+) â†’ Parquet export + parallel COPY (15-30 min)
â”‚  â””â”€ Hybrid analytics â†’ pg_duckdb (query in-place)
â”‚
â””â”€ Parquet Files (Python pipeline)
   â”œâ”€ Server-side â†’ pg_parquet COPY (simple, integrated)
   â”œâ”€ Client-side â†’ pgpq Arrow streaming (fastest)
   â”œâ”€ Incremental â†’ dlt or Airbyte (pipeline orchestration)
   â””â”€ Ad-hoc queries â†’ Parquet FDW (no ingestion)
```

### Critical Insight: pg_duckdb Game-Changer

The **pg_duckdb extension** (official DuckDB project) represents a paradigm shift for PostgreSQL analytics:

- **Embeds DuckDB's columnar engine** inside PostgreSQL
- **1500Ã— speedups** reported for analytical queries (vendor benchmarks)
- **Direct data lake access** - query Parquet/Iceberg/Delta on S3/GCS without loading
- **MotherDuck integration** - cloud compute and catalog sync
- **Best of both worlds** - OLTP in PostgreSQL, OLAP via DuckDB

**Use Case:** Keep transactional data in PostgreSQL tables, query analytical Parquet files in-place with DuckDB's vectorized executionâ€”no ETL required.

---

---

## Current Proposal Analysis

### Strengths

1. **Well-structured specification** - Clear separation of concerns across modules
2. **Comprehensive requirements** - Covers edge cases and error scenarios
3. **Zero-sync awareness** - Explicitly designed for Zero-first architecture
4. **Configuration-driven** - YAML-based approach is user-friendly
5. **Detailed performance targets** - Specific throughput goals (>50K rows/sec)
6. **Idempotency focus** - Designed for repeatable execution
7. **Validation strategy** - Multi-level verification (row counts, sample data, types)

### Critical Issues & Omissions

#### 1. **Incremental Updates Strategy (CRITICAL)**

**Issue:** Listed as "open question" but this is a core requirement based on the use case.

**Context:** The SQLite database is generated by a Python data pipeline and needs periodic updates. The proposal only handles full table reloads (truncate + reload).

**Missing:**
- Change detection mechanism (identify new/modified/deleted rows)
- Upsert strategy (INSERT ... ON CONFLICT UPDATE)
- Timestamp-based incremental loading
- Primary key-based differential sync
- Handling of deleted records in source

**Impact:** Without this, every data refresh requires full table reloads of 101M+ rows, which is inefficient and causes extended downtime.

**Recommendation:** Add incremental update support using:
```sql
-- Upsert pattern for PostgreSQL
INSERT INTO holdings_overview (cik, cusip, quarter, shares, value)
VALUES (...)
ON CONFLICT (cik, cusip, quarter) 
DO UPDATE SET 
  shares = EXCLUDED.shares,
  value = EXCLUDED.value,
  updated_at = CURRENT_TIMESTAMP;
```

#### 2. **Disk Space Requirements (SIGNIFICANT)**

**Issue:** Requires ~20GB for CSV staging files.

**Context:** 
- SQLite file: 6.7GB (7.2GB actual)
- CSV staging: ~20GB (3x expansion due to text format)
- Total temporary space: ~27GB

**Problems:**
- Unnecessary disk I/O overhead
- Cleanup complexity if migration fails mid-process
- Risk of disk full errors during export

**pgloader Alternative:** Direct SQLite â†’ PostgreSQL transfer eliminates CSV staging entirely, saving 20GB and reducing I/O operations by ~50%.

#### 3. **Type Mapping Edge Cases (MODERATE)**

**Issue:** Acknowledged as risk but underspecified.

**Missing Details:**
- SQLite's dynamic typing affinity rules
- Handling of mixed types in same column (SQLite allows this)
- BLOB data handling strategy
- Date/time format variations (SQLite stores as TEXT, INTEGER, or REAL)
- NULL vs empty string differences
- Case sensitivity differences (LIKE operator)
- Collation differences

**Example Problem:**
```sql
-- SQLite allows this (dynamic typing)
CREATE TABLE test (value INTEGER);
INSERT INTO test VALUES (123);
INSERT INTO test VALUES ('text');
INSERT INTO test VALUES (45.67);

-- PostgreSQL will reject mixed types during COPY
```

**pgloader Advantage:** Built-in type affinity detection and conversion rules handle these cases automatically.

#### 4. **Concurrent Access During Migration (CRITICAL)**

**Issue:** No strategy for handling app access during migration.

**Missing:**
- Lock management strategy
- Read-only mode during migration
- Zero-sync behavior during schema changes
- Rollback impact on active connections
- Downtime estimation and communication

**Recommendation:** 
- Use PostgreSQL transactions with appropriate isolation levels
- Implement maintenance mode flag in app
- Consider blue-green deployment for zero-downtime updates

#### 5. **Data Transformation / ETL Capabilities (MODERATE)**

**Issue:** Listed as "open question" but likely needed.

**Common Transformations Needed:**
- Computed columns (e.g., `full_name = first_name || ' ' || last_name`)
- Data cleansing (trim whitespace, normalize case)
- Value mapping (e.g., status codes to enums)
- Denormalization for performance
- Aggregations for summary tables

**Current Proposal:** No mechanism for transformations during migration.

**pgloader Advantage:** Built-in CAST and USING expressions for transformations:
```lisp
CAST column_name to type USING (expression)
```

#### 6. **Backup and Rollback Strategy (CRITICAL)**

**Issue:** Only mentioned as "open question" but essential for production.

**Missing:**
- Pre-migration backup procedure
- Rollback mechanism beyond DROP TABLE
- Point-in-time recovery strategy
- Validation failure handling
- Partial migration rollback

**Recommendation:**
```sql
-- Before migration
CREATE SCHEMA backup_20250117;
CREATE TABLE backup_20250117.investors AS SELECT * FROM investors;

-- After validation
DROP SCHEMA backup_20250117 CASCADE;
```

#### 7. **Character Encoding Issues (MODERATE)**

**Issue:** Not mentioned in proposal.

**Potential Problems:**
- SQLite default encoding may differ from PostgreSQL
- UTF-8 validation during transfer
- Byte order marks (BOM) in text fields
- Invalid UTF-8 sequences
- Emoji and special characters

**pgloader Advantage:** Built-in encoding detection and conversion with `encoding utf-8` option.

#### 8. **Performance Uncertainty (MODERATE)**

**Issue:** 2-hour estimate is rough; no benchmarking plan.

**Missing:**
- Baseline performance tests
- Bottleneck identification (CPU, disk I/O, network, PostgreSQL)
- Tuning strategy based on actual measurements
- Comparison with alternative approaches

**Recommendation:** Add Phase 0 for benchmarking:
- Test with 1M row sample
- Measure CSV export time
- Measure COPY import time
- Identify bottlenecks
- Extrapolate to full dataset

#### 9. **Error Recovery Granularity (MODERATE)**

**Issue:** Proposal handles table-level and shard-level resumability but not row-level errors.

**Scenario:**
```
Table with 10M rows, shard 3 of 8 fails at row 4,567,890 due to:
- Invalid UTF-8 sequence
- Value exceeds column width
- Type conversion error
```

**Current Proposal:** Entire shard fails and must be retried.

**pgloader Advantage:** Batch-level retry with automatic isolation of problematic rows. Failed rows are logged separately, and migration continues.

#### 10. **SQLite-Specific Features Not Addressed (MINOR)**

**Missing:**
- FTS5 (Full-Text Search) tables - proposal excludes but doesn't explain migration path
- WITHOUT ROWID tables - different internal structure
- Virtual tables - may not be transferable
- Attached databases - if SQLite uses ATTACH
- SQLite extensions - custom functions/collations

**Current Approach:** Excludes FTS5 shadow tables but doesn't provide alternative search strategy for PostgreSQL.

---

## pgloader Evaluation

### What is pgloader?

pgloader is a mature, open-source data loading tool specifically designed for migrating data into PostgreSQL from various sources (MySQL, SQLite, CSV, MS SQL, etc.). Written in Common Lisp, it's optimized for bulk data transfers and handles many edge cases automatically.

**Key Stats:**
- First released: 2007 (17+ years of development)
- GitHub stars: 5.4K+
- Active maintenance: Regular updates
- Production usage: Widely used in enterprise migrations

### pgloader Capabilities Relevant to This Project

#### 1. **Direct SQLite â†’ PostgreSQL Transfer**

```lisp
LOAD DATABASE
     FROM sqlite:///Users/yo_macbook/Documents/app_data/TR_05_DB/TR_05_SQLITE_FILE.db
     INTO postgresql://user:pass@localhost:5432/dbname
  
  INCLUDING ONLY TABLE NAMES MATCHING 'cik_md', 'cusip_md', 'holdings_overview'
  
  WITH create tables, create indexes, reset sequences,
       workers = 8, concurrency = 4,
       batch rows = 50000, batch size = 20MB
  
  SET work_mem to '64MB', maintenance_work_mem to '1GB';
```

**Advantages:**
- No CSV intermediate files (saves 20GB disk space)
- Fewer I/O operations (SQLite â†’ PostgreSQL direct)
- Built-in parallelism (workers + concurrency)
- Automatic type mapping

#### 2. **Selective Table and Column Migration**

```lisp
INCLUDING ONLY TABLE NAMES MATCHING 'cik_md', 'cusip_md', 'holdings_overview'
EXCLUDING TABLE NAMES MATCHING ~/^searches_/, 'sqlite_sequence'

-- Column filtering via TARGET COLUMNS
INTO postgresql://...
  TARGET TABLE 'public'.'investors' (cik, name, description)
```

**Meets Requirements:**
- âœ… Select specific tables
- âœ… Rename tables (cik_md â†’ investors)
- âœ… Filter columns
- âœ… Exclude patterns (FTS5 tables)

#### 3. **Built-in Performance Optimization**

**Parallelism:**
- `workers = N` - Number of worker threads (default: 4)
- `concurrency = N` - Number of concurrent tasks (default: 1)
- `batch rows = N` - Rows per batch (default: 25,000)
- `batch size = NMB` - Memory per batch (default: 20MB)

**Index Management:**
- `create indexes` - Create indexes after data load (faster)
- `drop indexes` - Drop existing indexes before load
- `create no indexes` - Skip index creation entirely (for Zero-sync)

**PostgreSQL Tuning:**
- `SET work_mem` - Memory for sorts/operations
- `SET maintenance_work_mem` - Memory for index builds
- `SET synchronous_commit = OFF` - Faster writes (less durable)

**Expected Performance:**
- Small tables (<10K rows): <5 seconds
- Medium tables (10K-1M rows): <2 minutes
- Large tables (>1M rows): >50K rows/second
- 101M row table: ~30-45 minutes (vs 90 minutes in proposal)

#### 4. **Robust Error Handling**

**Batch Retry Logic:**
```
Batch 1 (25K rows) â†’ Success
Batch 2 (25K rows) â†’ COPY error
  â†“
Retry Batch 2 in smaller chunks:
  Rows 25001-30000 â†’ Success
  Rows 30001-35000 â†’ Success
  Rows 35001-40000 â†’ Error (row 37,542 has invalid UTF-8)
    â†“
  Log problematic row, continue with remaining data
```

**Error Reporting:**
- Detailed logs with row numbers
- Separate file for rejected rows
- Summary statistics (rows loaded, rows rejected, time taken)

#### 5. **Type Mapping and Conversions**

**Automatic Mappings:**
```
SQLite          â†’ PostgreSQL
INTEGER         â†’ BIGINT
TEXT            â†’ TEXT
REAL            â†’ DOUBLE PRECISION
BLOB            â†’ BYTEA
DATETIME        â†’ TIMESTAMP
```

**Custom Overrides:**
```lisp
CAST column_name to NUMERIC(10,4)
CAST created_at to TIMESTAMP USING (datetime-to-timestamp created_at)
```

**Handles Edge Cases:**
- Dynamic typing affinity
- Mixed types in same column (with warnings)
- NULL vs empty string
- Date/time format variations
- Character encoding issues

#### 6. **Progress Tracking and Logging**

**Real-time Output:**
```
                    table name       read   imported     errors      total time
--------------------------------  ---------  ---------  ---------  --------------
                  fetch meta data          0          0          0         0.123s
                   Create Schemas          0          0          0         0.001s
                 Create SQL Types          0          0          0         0.002s
                    Create tables          0          3          0         0.015s
                   Set Table OIDs          0          3          0         0.003s
--------------------------------  ---------  ---------  ---------  --------------
                         cik_md       15234      15234          0         0.234s
                       cusip_md       89456      89456          0         1.123s
               holdings_overview  101386020  101385998         22      2847.456s
--------------------------------  ---------  ---------  ---------  --------------
            COPY Threads Completion          3          3          0      2848.567s
                     Create Indexes          0          0          0         0.000s
                    Reset Sequences          0          3          0         0.012s
--------------------------------  ---------  ---------  ---------  --------------
              Total import time  101490710  101490688         22      2848.579s
```

### pgloader Limitations

#### 1. **External Dependency**

**Issue:** pgloader is not a Node.js/Bun package.

**Installation:**
```bash
# macOS
brew install pgloader

# Linux (Debian/Ubuntu)
apt-get install pgloader

# Docker
docker run --rm -it dimitri/pgloader:latest pgloader --version
```

**Impact:**
- Adds system dependency to project
- Requires installation in CI/CD pipeline
- Version management complexity
- Not portable across all environments

**Mitigation:**
- Use Docker container for consistency
- Document installation in README
- Provide fallback to custom tool if pgloader unavailable

#### 2. **Configuration DSL Learning Curve**

**Issue:** pgloader uses its own DSL (Domain-Specific Language), not YAML.

**Example Complexity:**
```lisp
LOAD DATABASE
     FROM sqlite:///path/to/source.db
     INTO postgresql://user:pass@host:5432/db
  
  INCLUDING ONLY TABLE NAMES MATCHING 'table1', ~/pattern/
  EXCLUDING TABLE NAMES MATCHING 'temp_*'
  
  CAST column_name to type USING (expression)
  
  BEFORE LOAD DO
    $$ CREATE SCHEMA IF NOT EXISTS target; $$
  
  AFTER LOAD DO
    $$ ANALYZE; $$
  
  WITH create tables, create indexes, reset sequences,
       workers = 8, concurrency = 4,
       batch rows = 50000
  
  SET work_mem to '64MB';
```

**Mitigation:**
- Generate pgloader config from YAML (TypeScript wrapper)
- Provide templates for common scenarios
- Abstract complexity behind simple interface

#### 3. **Limited Zero-sync Integration**

**Issue:** pgloader doesn't generate Zero schema definitions.

**Still Need Custom Code For:**
- Zero schema TypeScript generation
- Relationship mapping (.one(), .many())
- Permission rules
- Database migration files for docker/migrations/

**Solution:** Hybrid approach (see recommendation below)

#### 4. **Less Control Over Process**

**Issue:** Can't customize every aspect of migration.

**Examples:**
- Can't inject custom validation logic mid-migration
- Can't implement custom retry strategies
- Can't add custom progress hooks
- Limited control over transaction boundaries

**Impact:** Minimal for this use case, as pgloader's defaults are sensible.

---

## DuckDB â†’ PostgreSQL Migration

### Overview

DuckDB is an embedded analytical database optimized for OLAP workloads with native Parquet support and vectorized execution. Migrating from DuckDB to PostgreSQL requires different strategies than SQLite due to DuckDB's columnar architecture and data lake integration capabilities.

### Migration Approaches

#### 1. DuckDB postgres_scanner Extension (RECOMMENDED for Direct Migration)

**What it is:** Official DuckDB extension that enables direct read/write access to PostgreSQL from DuckDB.

**How it works:**
```sql
-- In DuckDB
INSTALL postgres;
LOAD postgres;

ATTACH 'host=localhost port=5432 dbname=mydb user=user password=pass' 
  AS pg_db (TYPE postgres);

-- Write data to PostgreSQL
CREATE TABLE pg_db.public.target_table AS
  SELECT * FROM duckdb_table;

-- Or insert into existing table
INSERT INTO pg_db.public.target_table
  SELECT * FROM duckdb_table;
```

**Advantages:**
- âœ… Direct DuckDB â†’ PostgreSQL transfer (no intermediate files)
- âœ… No disk space overhead
- âœ… Supports transactions
- âœ… Binary copy mode for speed (`pg_use_binary_copy`)
- âœ… Can query PostgreSQL data from DuckDB for validation
- âœ… Bidirectional - read and write

**Performance:**
- **Small tables (<10K rows):** <1 second
- **Medium tables (10K-1M rows):** 1-5 minutes
- **Large tables (>1M rows):** ~50-80K rows/second
- **101M row table:** ~20-35 minutes

**Configuration Options:**
```sql
-- Enable binary copy for faster transfers
SET pg_use_binary_copy = true;

-- Adjust batch size
SET pg_batch_size = 100000;

-- Handle array types
SET pg_array_as_varchar = true;  -- If array mapping issues
```

**Type Mapping Considerations:**
- Timestamps: DuckDB microsecond precision â†’ PostgreSQL
- Arrays: May need `pg_array_as_varchar` setting
- UUIDs: Automatic conversion
- Nested types (STRUCT/MAP): Require flattening or JSON conversion

**When to Use:**
- âœ… One-time bulk migration from DuckDB to PostgreSQL
- âœ… When you have DuckDB database files
- âœ… When you need transactional guarantees during migration
- âœ… When you want to validate data by querying both databases

#### 2. Parquet Export + Parallel COPY (FASTEST for 100M+ Rows)

**What it is:** Export DuckDB tables to Parquet, then bulk load into PostgreSQL using parallel COPY operations.

**How it works:**
```sql
-- Step 1: Export from DuckDB with parallelism
COPY my_table TO 'out_dir/mytable.parquet' (
  FORMAT 'parquet',
  PER_THREAD_OUTPUT true,           -- Parallel writes
  FILE_SIZE_BYTES '512MB',          -- Split into manageable files
  COMPRESSION 'snappy',             -- Fast compression
  ROW_GROUP_SIZE_BYTES '128MB'      -- Optimize for PostgreSQL reads
);

-- Step 2: Load into PostgreSQL (multiple parallel processes)
-- Run N parallel psql processes, each loading different files
```

**Advantages:**
- âœ… Fastest for very large datasets (100M+ rows)
- âœ… Parquet files are reusable and portable
- âœ… Can parallelize PostgreSQL load (multiple COPY processes)
- âœ… Good for S3/cloud storage workflows
- âœ… Enables data validation before load

**Performance:**
- **DuckDB export:** Very fast with PER_THREAD_OUTPUT (10-15 min for 101M rows)
- **PostgreSQL load:** Scales with parallel COPY workers (10-20 min with 8 workers)
- **Total:** ~15-30 minutes for 101M rows

**Disk Space Requirements:**
- Parquet files: ~2-4GB (compressed, columnar format)
- Temporary space: Minimal compared to CSV approach

**PostgreSQL Optimization for Bulk Load:**
```sql
-- Before load: Create UNLOGGED table
CREATE UNLOGGED TABLE staging_table (...);

-- Drop indexes before load
DROP INDEX IF EXISTS idx_name;

-- Tune PostgreSQL settings (session-level)
SET maintenance_work_mem = '4GB';
SET max_wal_size = '50GB';
SET checkpoint_timeout = '30min';
SET synchronous_commit = 'off';

-- Load data (run multiple in parallel)
\copy staging_table FROM 'file1.parquet' WITH (FORMAT parquet)

-- After load: Convert to logged, rebuild indexes
ALTER TABLE staging_table SET LOGGED;
CREATE INDEX CONCURRENTLY idx_name ON staging_table(column);
ANALYZE staging_table;
```

**When to Use:**
- âœ… Very large datasets (100M+ rows) where speed is critical
- âœ… When you need reusable Parquet files for other systems
- âœ… When you have multiple CPU cores for parallel loading
- âœ… Cloud-native workflows (S3/GCS storage)

#### 3. DuckDB Binary COPY Format (FASTEST Raw Speed)

**What it is:** DuckDB can write PostgreSQL's native binary COPY format directly.

**How it works:**
```sql
-- In DuckDB: Write PostgreSQL binary format
COPY some_table TO 'data.bin' WITH (FORMAT postgres_binary);

-- In PostgreSQL: Load binary format
\copy target_table FROM 'data.bin' WITH (FORMAT BINARY)
```

**Advantages:**
- âœ… Minimal parsing overhead
- âœ… Fastest raw ingestion speed
- âœ… Direct format compatibility

**Performance:**
- **101M rows:** ~10-25 minutes (fastest option)

**Disadvantages:**
- âŒ Not reusable like Parquet (binary format is PostgreSQL-specific)
- âŒ Less portable
- âŒ Format compatibility must match PostgreSQL version

**When to Use:**
- âœ… Pure speed is the only concern
- âœ… One-time migration with no need for file reuse
- âœ… Same-version PostgreSQL compatibility

#### 4. pg_duckdb Extension (HYBRID Architecture)

**What it is:** PostgreSQL extension that embeds DuckDB's execution engine inside PostgreSQL.

**Purpose:** NOT for migration, but for **hybrid OLTP/OLAP architecture**.

**How it works:**
```sql
-- In PostgreSQL
CREATE EXTENSION pg_duckdb;

-- Query DuckDB files directly from PostgreSQL
SELECT * FROM read_parquet('s3://bucket/data.parquet');

-- Or create DuckDB-backed table
CREATE TABLE my_parquet USING duckdb 
  AS SELECT * FROM read_parquet('s3://bucket/data.parquet');

-- Enable DuckDB execution for analytical queries
SET duckdb.force_execution = true;
```

**Key Capabilities:**
- ğŸš€ **Accelerates analytical queries** - 1500Ã— speedups reported (vendor benchmarks)
- ğŸ“Š **Direct data lake access** - Query Parquet/Iceberg/Delta on S3/GCS/Azure
- ğŸ”„ **MotherDuck integration** - Cloud compute and catalog sync
- ğŸ¯ **Selective execution** - Route OLAP queries to DuckDB, OLTP to PostgreSQL

**Performance Claims (MotherDuck benchmarks):**
- TPC-DS query: 81.8s (PostgreSQL) â†’ 52ms (pg_duckdb) = **1500Ã— faster**
- Multi-hour queries â†’ sub-second with pg_duckdb
- Note: Vendor benchmarks; validate with your workload

**Architecture Pattern:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PostgreSQL (OLTP)               â”‚
â”‚  â€¢ Transactional data                   â”‚
â”‚  â€¢ Indexes, constraints                 â”‚
â”‚  â€¢ ACID guarantees                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”‚ pg_duckdb extension
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DuckDB Engine (OLAP)            â”‚
â”‚  â€¢ Vectorized execution                 â”‚
â”‚  â€¢ Columnar processing                  â”‚
â”‚  â€¢ Direct Parquet access               â”‚
â”‚  â€¢ S3/GCS/Azure integration            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**When to Use:**
- âœ… Hybrid workload (OLTP + OLAP in same database)
- âœ… Need to query Parquet files without loading into PostgreSQL
- âœ… Analytical queries on large datasets
- âœ… Data lake integration (S3/GCS)
- âŒ NOT for one-time migration (use postgres_scanner instead)

#### 5. duckdb_fdw (Foreign Data Wrapper)

**What it is:** Third-party FDW that connects PostgreSQL to DuckDB database files.

**How it works:**
```sql
-- In PostgreSQL
CREATE EXTENSION duckdb_fdw;

CREATE SERVER duckdb_srv FOREIGN DATA WRAPPER duckdb_fdw 
  OPTIONS (database '/path/to/my.db');

IMPORT FOREIGN SCHEMA public FROM SERVER duckdb_srv INTO local_schema;

-- Query DuckDB tables as foreign tables
SELECT * FROM local_schema.duckdb_table;
```

**Advantages:**
- âœ… Query DuckDB files directly from PostgreSQL
- âœ… No data copying required
- âœ… Good for read-heavy analytical queries

**Disadvantages:**
- âŒ Uses SQLite compatibility layer (not native DuckDB API)
- âŒ Limited pushdown compared to pg_duckdb
- âŒ Write support varies
- âŒ Performance depends on FDW implementation

**When to Use:**
- âœ… Need to expose DuckDB database files as foreign tables
- âœ… Read-only analytical queries
- âœ… Cannot install pg_duckdb (prefer FDW abstraction)

### DuckDB Migration Comparison Matrix

| Approach | Speed (101M rows) | Disk Space | Complexity | Reusability | Best For |
|----------|-------------------|------------|------------|-------------|----------|
| **postgres_scanner** | 20-35 min | 0GB | Low | N/A | Direct migration |
| **Parquet + parallel COPY** | 15-30 min | 2-4GB | Medium | High | Largest datasets, cloud |
| **Binary COPY** | 10-25 min | Temp files | Low | Low | Pure speed |
| **pg_duckdb** | N/A (not for migration) | 0GB | Medium | N/A | Hybrid analytics |
| **duckdb_fdw** | N/A (query only) | 0GB | Medium | N/A | Foreign table access |

### Recommended DuckDB Migration Strategy

**For this project (101M rows):**

1. **Primary approach:** DuckDB postgres_scanner
   - Direct, simple, transactional
   - 20-35 minute migration time
   - No intermediate files

2. **Alternative (if speed critical):** Parquet export + parallel COPY
   - 15-30 minute migration time
   - Reusable Parquet files
   - Better for cloud workflows

3. **Future consideration:** pg_duckdb for analytics
   - Keep transactional data in PostgreSQL
   - Query analytical Parquet files in-place
   - No ETL required for analytics

---

## Parquet â†’ PostgreSQL Migration (Python Pipeline)

### Overview

When Python data pipelines generate Parquet files directly (instead of DuckDB databases), different tools and strategies are optimal. This section covers server-side and client-side ingestion patterns.

### Migration Approaches

#### 1. pg_parquet Extension (RECOMMENDED for Server-Side)

**What it is:** PostgreSQL extension (CrunchyData) that implements COPY FROM/TO Parquet with object storage support.

**How it works:**
```sql
-- In PostgreSQL
CREATE EXTENSION pg_parquet;

-- Load from S3
COPY target_table FROM 's3://mybucket/path/data.parquet' 
  WITH (format 'parquet');

-- Load from GCS
COPY target_table FROM 'gs://mybucket/path/data.parquet' 
  WITH (format 'parquet');

-- Load from local file
COPY target_table FROM '/path/to/data.parquet' 
  WITH (format 'parquet');

-- Export to Parquet
COPY (SELECT * FROM source_table) TO 's3://mybucket/export.parquet' 
  WITH (format 'parquet');
```

**Advantages:**
- âœ… Native PostgreSQL COPY semantics
- âœ… S3/GCS/Azure/local support
- âœ… Server-side execution (no client data transfer)
- âœ… Simple, familiar COPY syntax
- âœ… Supports schema matching by name or position

**Performance:**
- **Small tables (<10K rows):** <1 second
- **Medium tables (10K-1M rows):** 1-5 minutes
- **Large tables (>1M rows):** ~40-60K rows/second
- **101M rows:** ~30-45 minutes (single COPY)

**Configuration Options:**
```sql
-- Match columns by name (default: position)
COPY table FROM 'file.parquet' WITH (format 'parquet', match_by_name 'true');

-- Specify row group size
COPY table TO 'file.parquet' WITH (format 'parquet', row_group_size '128MB');
```

**Installation:**
```bash
# Requires building with pgrx (Rust)
cargo install cargo-pgrx
cargo pgrx install --pg-config /path/to/pg_config
```

**When to Use:**
- âœ… Can install PostgreSQL extensions
- âœ… Files in object storage (S3/GCS/Azure)
- âœ… Prefer server-side execution
- âœ… Simple COPY-based workflow

#### 2. pgpq + PyArrow (FASTEST for Client-Side)

**What it is:** Python library that encodes PyArrow RecordBatches into PostgreSQL binary COPY format for streaming ingestion.

**How it works:**
```python
import pyarrow.parquet as pq
import psycopg
from pgpq import ArrowToPostgresBinaryEncoder

# Read Parquet file
table = pq.read_table('data.parquet')

# Connect to PostgreSQL
conn = psycopg.connect('postgresql://user:pass@localhost/db')

# Stream Arrow batches to PostgreSQL
with conn.cursor() as cur:
    with cur.copy("COPY target_table FROM STDIN WITH (FORMAT BINARY)") as copy:
        encoder = ArrowToPostgresBinaryEncoder(table.schema)
        
        # Stream in batches
        for batch in table.to_batches(max_chunksize=50000):
            copy.write(encoder.write_batch(batch))

conn.commit()
```

**Advantages:**
- âœ… Fastest client-side ingestion (binary COPY)
- âœ… Low memory overhead (streaming batches)
- âœ… Preserves Arrow types
- âœ… No intermediate CSV conversion
- âœ… Works with any PostgreSQL (no extensions required)

**Performance:**
- **Streaming overhead:** Minimal (binary format)
- **Network bound:** Depends on connection speed
- **101M rows:** ~25-40 minutes (single connection)
- **Parallel:** ~15-25 minutes (multiple connections)

**Parallel Loading Pattern:**
```python
from concurrent.futures import ThreadPoolExecutor
import pyarrow.dataset as ds

# Read Parquet dataset (multiple files)
dataset = ds.dataset('s3://bucket/path/', format='parquet')

def load_fragment(fragment):
    table = fragment.to_table()
    # ... pgpq streaming code ...

# Parallel load
with ThreadPoolExecutor(max_workers=8) as executor:
    executor.map(load_fragment, dataset.get_fragments())
```

**When to Use:**
- âœ… Cannot install PostgreSQL extensions
- âœ… Need custom transformation in Python
- âœ… Want fastest client-side ingestion
- âœ… Integrate with Python ETL orchestration (Airflow, Dagster, Prefect)

#### 3. Parquet FDW (Query In-Place)

**What it is:** Foreign Data Wrapper that allows PostgreSQL to query Parquet files without loading.

**How it works:**
```sql
-- In PostgreSQL
CREATE EXTENSION parquet_fdw;

CREATE SERVER parquet_srv FOREIGN DATA WRAPPER parquet_fdw;

CREATE FOREIGN TABLE my_parquet (
  id BIGINT,
  name TEXT,
  value NUMERIC
) SERVER parquet_srv
OPTIONS (filename '/path/to/data.parquet');

-- Query Parquet file directly
SELECT * FROM my_parquet WHERE value > 1000;
```

**Advantages:**
- âœ… No data ingestion required
- âœ… Good for ad-hoc queries
- âœ… Federated access to data lake
- âœ… Zero storage overhead in PostgreSQL

**Disadvantages:**
- âŒ Query performance depends on FDW implementation
- âŒ No indexes (full file scans)
- âŒ Limited pushdown optimization
- âŒ Not suitable for OLTP workloads

**When to Use:**
- âœ… Exploratory data analysis
- âœ… Joining Parquet data with PostgreSQL tables
- âœ… Avoid ingestion for infrequently accessed data
- âŒ NOT for production OLTP queries

#### 4. dlt (Data Load Tool) - Modern Pipeline Library

**What it is:** Python-first pipeline library with automatic schema inference and incremental loading.

**How it works:**
```python
import dlt
from dlt.sources.filesystem import filesystem

# Define pipeline
pipeline = dlt.pipeline(
    pipeline_name='parquet_to_postgres',
    destination='postgres',
    dataset_name='my_dataset'
)

# Load Parquet files
source = filesystem(
    bucket_url='s3://mybucket/path/',
    file_glob='*.parquet'
)

# Run pipeline (handles schema, incremental, etc.)
info = pipeline.run(source)
print(info)
```

**Advantages:**
- âœ… Automatic schema inference and evolution
- âœ… Built-in incremental loading (cursor-based)
- âœ… Native PostgreSQL COPY support
- âœ… Production-ready (monitoring, retries)
- âœ… Integrates with Airflow, notebooks, serverless

**Performance:**
- **101M rows:** ~40-60 minutes (includes schema management)
- **Incremental updates:** Very fast (only new/changed data)

**When to Use:**
- âœ… Need ongoing incremental syncs
- âœ… Schema evolution handling
- âœ… Pipeline orchestration
- âœ… Production data pipelines

#### 5. Airbyte (No-Code ELT Platform)

**What it is:** Open-source data integration platform with GUI for configuration.

**Advantages:**
- âœ… No-code configuration
- âœ… Cursor-based incremental sync
- âœ… Monitoring and scheduling built-in
- âœ… Supports append + deduped modes

**Disadvantages:**
- âŒ Heavier infrastructure (requires Airbyte server)
- âŒ No CDC for file sources (no delete capture)
- âŒ Requires cursor field (updated_at timestamp)

**When to Use:**
- âœ… Prefer GUI over code
- âœ… Need scheduled incremental syncs
- âœ… Team wants managed platform

### Parquet Migration Performance Tuning

#### PostgreSQL Settings (Temporary During Load)

```sql
-- Increase memory for operations
SET maintenance_work_mem = '4GB';

-- Reduce WAL overhead
SET max_wal_size = '50GB';
SET checkpoint_timeout = '30min';
SET synchronous_commit = 'off';

-- For non-replicated loads only
SET wal_level = 'minimal';  -- Requires restart
```

#### Staging Table Pattern

```sql
-- 1. Create UNLOGGED staging table (fast, no WAL)
CREATE UNLOGGED TABLE staging_table (...);

-- 2. Load data (no indexes, no constraints)
COPY staging_table FROM 's3://bucket/data.parquet' WITH (format 'parquet');

-- 3. Validate
SELECT COUNT(*) FROM staging_table;

-- 4. Move to production
INSERT INTO production_table SELECT * FROM staging_table;

-- 5. Convert to logged and add indexes
ALTER TABLE staging_table SET LOGGED;
CREATE INDEX CONCURRENTLY idx_name ON staging_table(column);
ANALYZE staging_table;
```

#### Parallel COPY Pattern

```bash
# Split Parquet files and load in parallel
for file in data_part_*.parquet; do
  psql -c "COPY staging_table FROM '$file' WITH (format 'parquet')" &
done
wait

# Or use GNU parallel
ls data_part_*.parquet | parallel -j 8 \
  "psql -c \"COPY staging_table FROM '{}' WITH (format 'parquet')\""
```

### Parquet Migration Comparison Matrix

| Approach | Speed (101M rows) | Setup | Extensions | Incremental | Best For |
|----------|-------------------|-------|------------|-------------|----------|
| **pg_parquet** | 30-45 min | Medium | Required | Custom | Server-side COPY |
| **pgpq + PyArrow** | 25-40 min (single)<br>15-25 min (parallel) | Low | None | Custom | Client-side streaming |
| **Parquet FDW** | N/A (query only) | Low | Required | N/A | Ad-hoc queries |
| **dlt** | 40-60 min | Medium | None | Built-in | Pipeline orchestration |
| **Airbyte** | 45-70 min | High | None | Built-in | No-code platform |

### Recommended Parquet Migration Strategy

**For this project (Python pipeline â†’ Parquet â†’ PostgreSQL):**

1. **If you can install extensions:** pg_parquet
   - Simple COPY syntax
   - Server-side execution
   - S3/GCS support

2. **If you cannot install extensions:** pgpq + PyArrow
   - Fastest client-side option
   - Binary COPY streaming
   - Works with any PostgreSQL

3. **If you need incremental updates:** dlt
   - Built-in incremental support
   - Schema evolution handling
   - Production-ready pipelines

4. **For ad-hoc analytics:** Parquet FDW or pg_duckdb
   - Query in-place without loading
   - No storage overhead
   - Good for exploration

---

## Comprehensive Tool Comparison Matrix

### Performance Comparison (101M Rows)

| Source â†’ Target | Tool | Time | Disk Space | Complexity | Maturity |
|-----------------|------|------|------------|------------|----------|
| **SQLite â†’ PostgreSQL** | pgloader | 30-45 min | 0GB | Low | Mature (17+ years) |
| | Custom CSV | 90+ min | 20GB | High | N/A |
| **DuckDB â†’ PostgreSQL** | postgres_scanner | 20-35 min | 0GB | Low | Mature |
| | Parquet + parallel COPY | 15-30 min | 2-4GB | Medium | Proven |
| | Binary COPY | 10-25 min | Temp | Low | Proven |
| **Parquet â†’ PostgreSQL** | pg_parquet | 30-45 min | 0GB | Medium | New (2023+) |
| | pgpq (single) | 25-40 min | 0GB | Low | Proven |
| | pgpq (parallel) | 15-25 min | 0GB | Medium | Proven |
| | dlt | 40-60 min | Minimal | Medium | Mature |
| | Airbyte | 45-70 min | Minimal | Low | Mature |

### Feature Comparison

| Feature | pgloader | postgres_scanner | pg_parquet | pgpq | pg_duckdb | dlt | Airbyte |
|---------|----------|------------------|------------|------|-----------|-----|---------|
| **Source Types** | SQLite, MySQL, MS SQL, CSV | DuckDB | Parquet | Parquet (Arrow) | Parquet, Iceberg, Delta | Many | Many |
| **Incremental Updates** | Custom | Custom | Custom | Custom | N/A | Built-in | Built-in |
| **Object Storage** | No | No | S3/GCS/Azure | Client-side | S3/GCS/Azure/R2 | S3/GCS/Azure | S3/GCS/Azure |
| **Extensions Required** | No | No | Yes | No | Yes | No | No |
| **Type Mapping** | Automatic | Automatic | Automatic | Manual | Automatic | Automatic | Automatic |
| **Error Recovery** | Batch retry | Transaction | COPY | Custom | N/A | Built-in | Built-in |
| **Parallel Loading** | Built-in | Single | Single | Custom | N/A | Built-in | Built-in |
| **Schema Generation** | Yes | No | No | No | No | Yes | Yes |
| **GUI** | No | No | No | No | No | No | Yes |
| **Analytics In-Place** | No | No | No | No | **Yes** | No | No |

### Decision Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MIGRATION DECISION TREE                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. What is your source data format?
   â”œâ”€ SQLite database â†’ pgloader (proven, 30-45 min)
   â”œâ”€ DuckDB database â†’ postgres_scanner (direct, 20-35 min)
   â””â”€ Parquet files â†’ Continue to #2

2. Can you install PostgreSQL extensions?
   â”œâ”€ YES â†’ pg_parquet (server-side, 30-45 min)
   â””â”€ NO â†’ pgpq + PyArrow (client-side, 15-40 min)

3. Do you need incremental updates?
   â”œâ”€ YES â†’ dlt or Airbyte (built-in incremental)
   â””â”€ NO â†’ One-time bulk load (see #1-2)

4. Do you need analytics on Parquet without loading?
   â”œâ”€ YES â†’ pg_duckdb (query in-place, 1500Ã— speedups)
   â””â”€ NO â†’ Standard migration (see #1-3)

5. What is your data volume?
   â”œâ”€ <10M rows â†’ Any tool works well
   â”œâ”€ 10M-100M rows â†’ postgres_scanner or pg_parquet
   â””â”€ >100M rows â†’ Parquet + parallel COPY (15-30 min)

6. What is your team's preference?
   â”œâ”€ Code-first â†’ pgloader, postgres_scanner, pgpq, dlt
   â”œâ”€ GUI-first â†’ Airbyte
   â””â”€ Hybrid â†’ dlt (code) + Airbyte (monitoring)
```

### Cost-Benefit Analysis

| Approach | Dev Time | Migration Time | Disk Space | Maintenance | Total Cost |
|----------|----------|----------------|------------|-------------|------------|
| **Custom CSV (original proposal)** | 40-60 hrs | 90 min | 20GB | High | **HIGH** |
| **pgloader (SQLite)** | 8-12 hrs | 30-45 min | 0GB | Low | **LOW** |
| **postgres_scanner (DuckDB)** | 4-8 hrs | 20-35 min | 0GB | Low | **LOW** |
| **pg_parquet (Parquet)** | 8-12 hrs | 30-45 min | 0GB | Medium | **MEDIUM** |
| **pgpq (Parquet)** | 8-12 hrs | 15-40 min | 0GB | Low | **LOW** |
| **dlt (Incremental)** | 12-16 hrs | 40-60 min | Minimal | Low | **MEDIUM** |
| **Airbyte (No-code)** | 4-8 hrs | 45-70 min | Minimal | Low | **LOW** |

---

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TypeScript Wrapper                        â”‚
â”‚  (scripts/sqlite-to-postgres/migrate.ts)                    â”‚
â”‚                                                              â”‚
â”‚  â€¢ Load YAML configuration                                  â”‚
â”‚  â€¢ Validate source/target                                   â”‚
â”‚  â€¢ Generate pgloader config                                 â”‚
â”‚  â€¢ Execute pgloader                                         â”‚
â”‚  â€¢ Validate results                                         â”‚
â”‚  â€¢ Generate Zero schema                                     â”‚
â”‚  â€¢ Generate database migrations                             â”‚
â”‚  â€¢ Report summary                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        pgloader                              â”‚
â”‚  (External tool via Docker or system install)               â”‚
â”‚                                                              â”‚
â”‚  â€¢ Read SQLite schema                                       â”‚
â”‚  â€¢ Map types to PostgreSQL                                  â”‚
â”‚  â€¢ Transfer data with parallelism                           â”‚
â”‚  â€¢ Handle errors with batch retry                           â”‚
â”‚  â€¢ Create indexes (optional)                                â”‚
â”‚  â€¢ Report progress                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PostgreSQL Database                       â”‚
â”‚  (Target for migrated data)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Plan

#### Phase 1: Foundation (2-3 hours)

**T1.1** Create migration scripts directory structure
```
scripts/sqlite-to-postgres/
â”œâ”€â”€ migrate.ts              # Main orchestrator
â”œâ”€â”€ config.ts               # YAML config loader
â”œâ”€â”€ pgloader-generator.ts   # Generate pgloader config from YAML
â”œâ”€â”€ validator.ts            # Post-migration validation
â”œâ”€â”€ zero-generator.ts       # Zero schema generator
â””â”€â”€ templates/
    â”œâ”€â”€ pgloader.template   # pgloader config template
    â””â”€â”€ zero-schema.template # Zero schema template
```

**T1.2** Define YAML configuration schema (same as proposal)
```yaml
source:
  path: /path/to/sqlite.db
  
target:
  connection: $ZERO_UPSTREAM_DB
  schema: public

tables:
  - name: cik_md
    target_name: investors
    columns: [cik, name, description]
    
  - name: holdings_overview
    batch_size: 50000
    parallel_workers: 8

exclude_tables:
  - searches*
  - sqlite_sequence
```

**T1.3** Implement pgloader config generator
```typescript
function generatePgloaderConfig(config: MigrationConfig): string {
  return `
LOAD DATABASE
     FROM sqlite://${config.source.path}
     INTO ${config.target.connection}
  
  INCLUDING ONLY TABLE NAMES MATCHING ${config.tables.map(t => `'${t.name}'`).join(', ')}
  EXCLUDING TABLE NAMES MATCHING ${config.exclude_tables.map(t => `'${t}'`).join(', ')}
  
  WITH create tables, create no indexes, reset sequences,
       workers = ${config.performance.workers},
       concurrency = ${config.performance.concurrency},
       batch rows = ${config.performance.batch_rows}
  
  SET work_mem to '${config.performance.work_mem}',
      maintenance_work_mem to '${config.performance.maintenance_work_mem}';
  `;
}
```

#### Phase 2: Execution & Validation (2-3 hours)

**T2.1** Implement pgloader execution
```typescript
async function executePgloader(configPath: string): Promise<PgloaderResult> {
  // Use Docker for consistency
  const result = await exec(`
    docker run --rm -v ${configPath}:/config.load dimitri/pgloader:latest \
      pgloader /config.load
  `);
  
  return parsePgloaderOutput(result.stdout);
}
```

**T2.2** Implement validation (reuse from proposal)
- Row count comparison
- Sample data verification
- Type validation

**T2.3** Add error handling and retry logic

#### Phase 3: Zero Integration (2-3 hours)

**T3.1** Generate Zero schema definitions (reuse from proposal)
**T3.2** Generate database migration files
**T3.3** Generate relationship mappings

#### Phase 4: Testing & Documentation (2-3 hours)

**T4.1** End-to-end test with sample data
**T4.2** Write migration guide
**T4.3** Add CLI help and examples

**Total Effort:** 8-12 hours (vs 40-60 hours for full custom implementation)

### Benefits of Hybrid Approach

1. **70% Less Development Time**
   - Leverage pgloader's existing functionality
   - Focus on integration and validation
   - Reduce testing burden

2. **Zero Disk Space for Staging**
   - No CSV intermediate files
   - Direct SQLite â†’ PostgreSQL transfer
   - Reduced I/O operations

3. **Battle-Tested Performance**
   - pgloader is optimized for large datasets
   - Proven in production environments
   - Handles edge cases automatically

4. **Maintains Flexibility**
   - YAML configuration (user-friendly)
   - TypeScript wrapper (customizable)
   - Zero-sync integration (project-specific)

5. **Easier Maintenance**
   - Less custom code to maintain
   - pgloader updates handled by maintainers
   - Clear separation of concerns

6. **Better Error Handling**
   - Batch retry logic built-in
   - Detailed error reporting
   - Automatic isolation of bad rows

### Comparison Table

| Feature | Custom CSV Approach | pgloader Approach | Hybrid Approach |
|---------|-------------------|------------------|-----------------|
| Development Time | 40-60 hours | N/A (external tool) | 8-12 hours |
| Disk Space Required | ~20GB (CSV staging) | 0GB (direct transfer) | 0GB (direct transfer) |
| Performance (101M rows) | ~90 minutes (estimated) | ~30-45 minutes (proven) | ~30-45 minutes |
| Type Mapping | Custom implementation | Built-in, battle-tested | Built-in |
| Error Handling | Custom implementation | Batch retry, robust | Batch retry + custom validation |
| Parallelism | Custom sharding logic | Built-in workers/concurrency | Built-in |
| Progress Tracking | Custom implementation | Built-in, detailed | Built-in + custom reporting |
| Zero Integration | Native | None | Custom wrapper |
| Maintenance Burden | High (all custom code) | Low (external tool) | Low (thin wrapper) |
| Learning Curve | Low (TypeScript) | Medium (pgloader DSL) | Low (YAML + wrapper) |
| Flexibility | High | Medium | High |
| Resumability | Custom state management | Built-in batch tracking | Built-in + custom state |
| Incremental Updates | Need to implement | Not supported | Need to implement |

---

## Specific Recommendations

### 1. **Adopt Hybrid Approach**

**Action:** Use pgloader for data transfer, TypeScript wrapper for integration.

**Rationale:**
- Reduces development time by 70%
- Eliminates 20GB disk space requirement
- Provides proven performance and error handling
- Maintains flexibility for Zero-sync integration

**Implementation:**
- Phase 1: Build TypeScript wrapper (8-12 hours)
- Phase 2: Test with sample data (2-3 hours)
- Phase 3: Production migration (1-2 hours)

### 2. **Add Incremental Update Support**

**Action:** Implement upsert strategy for data refreshes.

**Approach:**
```typescript
// After initial migration with pgloader, use upsert for updates
async function incrementalUpdate(table: string, data: any[]) {
  await db.query(`
    INSERT INTO ${table} (${columns.join(', ')})
    VALUES ${data.map(row => `(${row.values.join(', ')})`).join(', ')}
    ON CONFLICT (${primaryKey})
    DO UPDATE SET
      ${updateColumns.map(col => `${col} = EXCLUDED.${col}`).join(', ')},
      updated_at = CURRENT_TIMESTAMP
  `);
}
```

**Benefits:**
- Faster data refreshes (only changed rows)
- Reduced downtime
- Preserves existing data

### 3. **Implement Backup Strategy**

**Action:** Add pre-migration backup and rollback mechanism.

**Approach:**
```sql
-- Before migration
CREATE SCHEMA backup_${timestamp};
CREATE TABLE backup_${timestamp}.${table} AS SELECT * FROM ${table};

-- After validation
DROP SCHEMA backup_${timestamp} CASCADE;
```

**Benefits:**
- Safe rollback on validation failure
- Point-in-time recovery
- Confidence in production migrations

### 4. **Add Concurrent Access Strategy**

**Action:** Implement maintenance mode during migration.

**Approach:**
```typescript
// Set maintenance flag
await redis.set('maintenance_mode', 'true');

// Run migration
await executeMigration();

// Validate
await validateMigration();

// Clear maintenance flag
await redis.del('maintenance_mode');
```

**Benefits:**
- Prevents data corruption
- Clear communication to users
- Controlled downtime

### 5. **Create Benchmarking Phase**

**Action:** Add Phase 0 for performance testing.

**Approach:**
```typescript
// Test with 1M row sample
const sample = await createSample(1_000_000);
const startTime = Date.now();
await executeMigration(sample);
const duration = Date.now() - startTime;

// Extrapolate to full dataset
const estimatedTime = (duration / 1_000_000) * 101_386_020;
console.log(`Estimated full migration time: ${estimatedTime}ms`);
```

**Benefits:**
- Accurate time estimates
- Identify bottlenecks early
- Tune performance before production run

### 6. **Document SQLite-Specific Considerations**

**Action:** Add section to migration guide covering:
- FTS5 table exclusion and PostgreSQL alternative (pg_trgm, tsquery)
- Dynamic typing edge cases
- Date/time format handling
- Character encoding validation
- BLOB data considerations

### 7. **Add Data Transformation Support**

**Action:** Support CAST and USING expressions in YAML config.

**Approach:**
```yaml
tables:
  - name: investors
    transformations:
      - column: name
        cast: TEXT
        using: "TRIM(UPPER(name))"
      - column: created_at
        cast: TIMESTAMP
        using: "datetime-to-timestamp(created_at)"
```

**Benefits:**
- Data cleansing during migration
- Computed columns
- Type conversions

---

## Migration Path Forward

### Immediate Actions (Week 1)

1. **Decision:** Approve hybrid approach or stick with custom CSV approach
2. **Prototype:** Build minimal TypeScript wrapper + pgloader integration
3. **Test:** Run migration on 1M row sample
4. **Measure:** Compare performance, disk usage, error handling

### Short-term (Week 2-3)

1. **Implement:** Full TypeScript wrapper with validation
2. **Test:** End-to-end migration with full dataset
3. **Document:** Migration guide and troubleshooting
4. **Deploy:** Production migration with backup strategy

### Long-term (Month 2+)

1. **Implement:** Incremental update support
2. **Optimize:** Performance tuning based on production metrics
3. **Automate:** Scheduled data refreshes from Python pipeline
4. **Monitor:** Track migration performance and errors

---

## Conclusion

This comprehensive analysis evaluated migration strategies from three source types (SQLite, DuckDB, Parquet) to PostgreSQL, revealing significant opportunities for optimization and architectural innovation.

### Key Findings Summary

#### 1. SQLite â†’ PostgreSQL (Original Scope)

The current OpenSpec proposal is well-structured but reinvents functionality that pgloader already provides in a mature, optimized form. The custom CSV approach would require 40-60 hours of development, 20GB of disk space, and ongoing maintenance burden.

**Recommended Approach:**
- Use pgloader for data transfer (proven, optimized, robust)
- Build thin TypeScript wrapper for configuration and Zero-sync integration
- Implement incremental update support for data refreshes
- Add backup and rollback strategy for production safety

**Expected Outcomes:**
- 70% reduction in development time (8-12 hours vs 40-60 hours)
- Zero disk space for CSV staging (saves 20GB)
- 50% faster migration (30-45 minutes vs 90 minutes)
- Better error handling (batch retry built-in)
- Easier maintenance (less custom code)

#### 2. DuckDB â†’ PostgreSQL (New Analysis)

DuckDB sources offer faster migration paths than SQLite due to native Parquet support and vectorized execution.

**Recommended Approach:**
- **Primary:** DuckDB postgres_scanner extension (20-35 min, direct transfer)
- **Alternative:** Parquet export + parallel COPY (15-30 min, fastest for 100M+ rows)
- **Future:** pg_duckdb extension for hybrid OLTP/OLAP architecture

**Key Insight:** pg_duckdb enables querying Parquet files in-place without loading into PostgreSQLâ€”a paradigm shift for analytics workloads.

#### 3. Parquet â†’ PostgreSQL (New Analysis)

When Python pipelines generate Parquet files, multiple high-performance ingestion paths exist.

**Recommended Approach:**
- **Server-side:** pg_parquet extension (30-45 min, COPY-based)
- **Client-side:** pgpq + PyArrow (15-40 min, binary streaming)
- **Incremental:** dlt or Airbyte (built-in incremental support)
- **Analytics:** Parquet FDW or pg_duckdb (query in-place)

**Key Insight:** Binary COPY streaming (pgpq) is 2-3Ã— faster than CSV-based approaches.

### Strategic Recommendations by Scenario

#### Scenario A: One-Time Migration (Current Project)

**Source: SQLite (6.7GB, 101M rows)**
- **Tool:** pgloader with TypeScript wrapper
- **Time:** 30-45 minutes
- **Dev effort:** 8-12 hours
- **Disk space:** 0GB (no staging)

#### Scenario B: Python Pipeline Generates DuckDB

**Source: DuckDB database files**
- **Tool:** DuckDB postgres_scanner
- **Time:** 20-35 minutes
- **Dev effort:** 4-8 hours
- **Disk space:** 0GB (direct transfer)

**Alternative (speed critical):**
- **Tool:** Parquet export + parallel COPY
- **Time:** 15-30 minutes
- **Dev effort:** 8-12 hours
- **Disk space:** 2-4GB (temporary Parquet)

#### Scenario C: Python Pipeline Generates Parquet

**Source: Parquet files on S3/GCS/local**
- **Tool:** pg_parquet (if extensions allowed) or pgpq (if not)
- **Time:** 15-45 minutes (depending on parallelism)
- **Dev effort:** 8-12 hours
- **Disk space:** 0GB (streaming)

**For incremental updates:**
- **Tool:** dlt (Python pipeline library)
- **Time:** 40-60 min (initial), <5 min (incremental)
- **Dev effort:** 12-16 hours
- **Benefit:** Automatic schema evolution, cursor-based incremental

### Game-Changing Technology: pg_duckdb

The **pg_duckdb extension** represents a fundamental shift in PostgreSQL analytics capabilities:

**What it enables:**
- Query Parquet/Iceberg/Delta files directly from PostgreSQL
- 1500Ã— speedups for analytical queries (vendor benchmarks)
- Hybrid architecture: OLTP in PostgreSQL, OLAP via DuckDB
- No ETL required for analytics on data lakes

**Recommended architecture:**
```
PostgreSQL (OLTP)
  â”œâ”€ Transactional tables (investors, assets, holdings)
  â””â”€ pg_duckdb extension
       â””â”€ Query Parquet files on S3 (historical data, analytics)
```

**Benefits:**
- Keep hot data in PostgreSQL (fast OLTP)
- Keep cold data in Parquet on S3 (cheap storage)
- Query both seamlessly with DuckDB's vectorized engine
- No data duplication or ETL pipelines

### Performance Comparison (101M Rows)

| Source | Tool | Time | Disk | Dev Hours | Maturity |
|--------|------|------|------|-----------|----------|
| SQLite | Custom CSV | 90 min | 20GB | 40-60 | N/A |
| SQLite | **pgloader** | **30-45 min** | **0GB** | **8-12** | **Mature** |
| DuckDB | postgres_scanner | 20-35 min | 0GB | 4-8 | Mature |
| DuckDB | Parquet + parallel | **15-30 min** | 2-4GB | 8-12 | Proven |
| DuckDB | Binary COPY | **10-25 min** | Temp | 4-8 | Proven |
| Parquet | pg_parquet | 30-45 min | 0GB | 8-12 | New (2023+) |
| Parquet | pgpq (parallel) | **15-25 min** | 0GB | 8-12 | Proven |
| Parquet | dlt | 40-60 min | Min | 12-16 | Mature |

**Fastest options:**
1. DuckDB Binary COPY: 10-25 min (pure speed, one-time)
2. Parquet + parallel COPY: 15-30 min (reusable, cloud-native)
3. pgpq parallel streaming: 15-25 min (client-side, no extensions)

### Critical Additions Needed (All Approaches)

1. **Incremental update strategy** - Use dlt or Airbyte for ongoing syncs
2. **Backup and rollback mechanism** - Schema-based backups before migration
3. **Concurrent access handling** - Maintenance mode during migration
4. **Benchmarking phase** - Validate performance with sample data
5. **Type mapping validation** - Test edge cases before full migration
6. **Monitoring and observability** - Track migration progress and errors

### Future-Proofing Recommendations

1. **Evaluate pg_duckdb for analytics** - Potential 1500Ã— speedups for OLAP queries
2. **Consider Parquet-based architecture** - Cheaper storage, faster analytics
3. **Implement incremental pipelines** - Use dlt or Airbyte for ongoing updates
4. **Adopt hybrid OLTP/OLAP** - PostgreSQL for transactions, DuckDB for analytics
5. **Leverage object storage** - S3/GCS for historical data, PostgreSQL for hot data

### Final Recommendation

**For this project (SQLite â†’ PostgreSQL):**
- **Use pgloader** with TypeScript wrapper (proven, 30-45 min, 8-12 hrs dev)
- **Add incremental support** using dlt or custom upsert logic
- **Evaluate pg_duckdb** for future analytics workloads

**For future projects:**
- **DuckDB sources:** Use postgres_scanner (20-35 min, simplest)
- **Parquet sources:** Use pgpq if no extensions, pg_parquet if allowed
- **Incremental pipelines:** Use dlt (built-in incremental, schema evolution)
- **Analytics:** Use pg_duckdb (query Parquet in-place, no ETL)

The hybrid approach provides the best balance of development efficiency, performance, maintainability, and flexibility while positioning the project for future analytical capabilities through pg_duckdb integration.

---

## Appendix: pgloader Example Configuration

```lisp
LOAD DATABASE
     FROM sqlite:///Users/yo_macbook/Documents/app_data/TR_05_DB/TR_05_SQLITE_FILE.db
     INTO postgresql://user:pass@localhost:5432/zapp
  
  INCLUDING ONLY TABLE NAMES MATCHING 
    'cik_md', 'cusip_md', 'holdings_overview', 
    'periods', 'reported_qtrs', 'every_qtr',
    'high_level_totals', 'twrr_per_cik_per_qtr',
    'assets', 'superinvestors'
  
  EXCLUDING TABLE NAMES MATCHING 
    ~/^searches/, 'sqlite_sequence'
  
  BEFORE LOAD DO
    $$ DROP SCHEMA IF EXISTS backup CASCADE; $$,
    $$ CREATE SCHEMA backup; $$,
    $$ CREATE TABLE backup.investors AS SELECT * FROM investors WHERE 1=0; $$
  
  WITH 
    create tables,
    create no indexes,
    reset sequences,
    workers = 8,
    concurrency = 4,
    batch rows = 50000,
    batch size = 20MB,
    prefetch rows = 100000
  
  CAST 
    column cik_md.cik to BIGINT,
    column holdings_overview.shares to BIGINT,
    column holdings_overview.value to NUMERIC(15,2)
  
  SET 
    work_mem to '64MB',
    maintenance_work_mem to '1GB',
    synchronous_commit to 'off'
  
  AFTER LOAD DO
    $$ ANALYZE; $$;
```

**Estimated Performance:**
- cik_md (15K rows): <1 second
- cusip_md (89K rows): ~2 seconds
- holdings_overview (101M rows): ~30-45 minutes
- Other tables (small): <5 seconds each
- **Total: ~45-60 minutes** (vs 2 hours in proposal)

---

## Appendix B: DuckDB postgres_scanner Example

```sql
-- In DuckDB CLI or Python
INSTALL postgres;
LOAD postgres;

-- Attach PostgreSQL database
ATTACH 'host=localhost port=5432 dbname=zapp user=postgres password=pass' 
  AS pg_db (TYPE postgres);

-- Enable binary copy for performance
SET pg_use_binary_copy = true;
SET pg_batch_size = 100000;

-- Migrate tables
CREATE TABLE pg_db.public.investors AS 
  SELECT * FROM cik_md;

CREATE TABLE pg_db.public.assets AS 
  SELECT * FROM cusip_md;

CREATE TABLE pg_db.public.holdings AS 
  SELECT * FROM holdings_overview;

-- Verify migration
SELECT COUNT(*) FROM pg_db.public.holdings;

-- Detach when done
DETACH pg_db;
```

**Estimated Performance:**
- cik_md (15K rows): <1 second
- cusip_md (89K rows): ~2 seconds
- holdings_overview (101M rows): ~20-35 minutes
- **Total: ~20-35 minutes**

---

## Appendix C: Parquet Export + Parallel COPY Example

### Step 1: Export from DuckDB to Parquet

```sql
-- In DuckDB
COPY holdings_overview TO 'output/holdings' (
  FORMAT 'parquet',
  PER_THREAD_OUTPUT true,           -- Creates multiple files
  FILE_SIZE_BYTES '512MB',          -- 512MB per file
  COMPRESSION 'snappy',             -- Fast compression
  ROW_GROUP_SIZE_BYTES '128MB'      -- Optimize for reading
);

-- This creates: holdings_0.parquet, holdings_1.parquet, etc.
```

### Step 2: Parallel Load into PostgreSQL

```bash
#!/bin/bash
# parallel-load.sh

# Prepare PostgreSQL
psql -c "CREATE UNLOGGED TABLE staging_holdings (...);"
psql -c "DROP INDEX IF EXISTS idx_holdings_cik;"

# Load files in parallel (8 workers)
ls output/holdings_*.parquet | parallel -j 8 \
  "psql -c \"COPY staging_holdings FROM '{}' WITH (FORMAT parquet)\""

# Post-load optimization
psql -c "ALTER TABLE staging_holdings SET LOGGED;"
psql -c "CREATE INDEX CONCURRENTLY idx_holdings_cik ON staging_holdings(cik);"
psql -c "ANALYZE staging_holdings;"
```

**Estimated Performance:**
- Export (DuckDB): ~10-15 minutes
- Load (PostgreSQL, 8 parallel): ~10-20 minutes
- **Total: ~15-30 minutes**

---

## Appendix D: pgpq + PyArrow Streaming Example

```python
#!/usr/bin/env python3
"""
Stream Parquet to PostgreSQL using pgpq (Arrow binary COPY)
"""
import pyarrow.parquet as pq
import pyarrow.dataset as ds
import psycopg
from pgpq import ArrowToPostgresBinaryEncoder
from concurrent.futures import ThreadPoolExecutor
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_parquet_file(file_path: str, conn_string: str, table_name: str):
    """Load single Parquet file into PostgreSQL"""
    logger.info(f"Loading {file_path}...")
    
    # Read Parquet file
    table = pq.read_table(file_path)
    
    # Connect to PostgreSQL
    with psycopg.connect(conn_string) as conn:
        with conn.cursor() as cur:
            # Stream Arrow batches to PostgreSQL
            with cur.copy(f"COPY {table_name} FROM STDIN WITH (FORMAT BINARY)") as copy:
                encoder = ArrowToPostgresBinaryEncoder(table.schema)
                
                # Stream in batches (50K rows per batch)
                for batch in table.to_batches(max_chunksize=50000):
                    copy.write(encoder.write_batch(batch))
        
        conn.commit()
    
    logger.info(f"Completed {file_path}")

def load_parquet_dataset_parallel(
    dataset_path: str,
    conn_string: str,
    table_name: str,
    max_workers: int = 8
):
    """Load Parquet dataset in parallel"""
    # Read dataset (multiple files)
    dataset = ds.dataset(dataset_path, format='parquet')
    
    # Get all fragments (files)
    fragments = list(dataset.get_fragments())
    logger.info(f"Found {len(fragments)} Parquet files")
    
    # Load in parallel
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for fragment in fragments:
            table = fragment.to_table()
            future = executor.submit(
                load_parquet_file,
                fragment.path,
                conn_string,
                table_name
            )
            futures.append(future)
        
        # Wait for all to complete
        for future in futures:
            future.result()
    
    logger.info("All files loaded successfully")

if __name__ == "__main__":
    # Configuration
    DATASET_PATH = "s3://mybucket/holdings/"  # or local path
    CONN_STRING = "postgresql://user:pass@localhost:5432/zapp"
    TABLE_NAME = "staging_holdings"
    
    # Load dataset
    load_parquet_dataset_parallel(
        DATASET_PATH,
        CONN_STRING,
        TABLE_NAME,
        max_workers=8
    )
```

**Estimated Performance:**
- Single connection: ~25-40 minutes
- Parallel (8 workers): ~15-25 minutes

---

## Appendix E: pg_parquet Extension Example

```sql
-- Install extension (requires pgrx/Rust build)
CREATE EXTENSION pg_parquet;

-- Load from S3
COPY holdings FROM 's3://mybucket/data/holdings.parquet' 
  WITH (format 'parquet');

-- Load from GCS
COPY holdings FROM 'gs://mybucket/data/holdings.parquet' 
  WITH (format 'parquet');

-- Load from local file
COPY holdings FROM '/path/to/holdings.parquet' 
  WITH (format 'parquet');

-- Match columns by name (instead of position)
COPY holdings FROM 's3://mybucket/data/holdings.parquet' 
  WITH (format 'parquet', match_by_name 'true');

-- Export to Parquet
COPY (SELECT * FROM holdings WHERE quarter >= '2024-01-01') 
  TO 's3://mybucket/export/holdings_2024.parquet' 
  WITH (format 'parquet', row_group_size '128MB');
```

**Estimated Performance:**
- 101M rows: ~30-45 minutes (single COPY)

---

## Appendix F: pg_duckdb Extension Example

```sql
-- Install extension
CREATE EXTENSION pg_duckdb;

-- Query Parquet files directly from PostgreSQL
SELECT * FROM read_parquet('s3://mybucket/historical/holdings_*.parquet')
WHERE quarter >= '2020-01-01'
LIMIT 100;

-- Create DuckDB-backed table
CREATE TABLE historical_holdings USING duckdb 
  AS SELECT * FROM read_parquet('s3://mybucket/historical/*.parquet');

-- Query Iceberg tables
SELECT * FROM iceberg_scan('s3://mybucket/iceberg/holdings');

-- Enable DuckDB execution for analytical queries
SET duckdb.force_execution = true;

-- This query will use DuckDB's vectorized engine
SELECT 
  quarter,
  COUNT(*) as num_holdings,
  SUM(value) as total_value,
  AVG(shares) as avg_shares
FROM holdings
WHERE quarter >= '2020-01-01'
GROUP BY quarter
ORDER BY quarter;

-- Disable DuckDB execution
SET duckdb.force_execution = false;

-- Hybrid query: Join PostgreSQL table with Parquet file
SELECT 
  i.name,
  h.quarter,
  SUM(h.value) as total_value
FROM investors i
JOIN read_parquet('s3://mybucket/holdings/*.parquet') h
  ON i.cik = h.cik
WHERE h.quarter >= '2024-01-01'
GROUP BY i.name, h.quarter
ORDER BY total_value DESC
LIMIT 10;
```

**Use Cases:**
- Query historical data in Parquet without loading
- Accelerate analytical queries (1500Ã— speedups reported)
- Hybrid OLTP/OLAP architecture
- Data lake integration

---

## Appendix G: Tool Installation Guide

### pgloader

```bash
# macOS
brew install pgloader

# Linux (Debian/Ubuntu)
apt-get install pgloader

# Docker
docker pull dimitri/pgloader:latest
docker run --rm -it dimitri/pgloader:latest pgloader --version
```

### DuckDB with postgres_scanner

```bash
# Install DuckDB CLI
brew install duckdb  # macOS
# or download from https://duckdb.org/

# In DuckDB
INSTALL postgres;
LOAD postgres;
```

### pg_parquet

```bash
# Requires Rust and pgrx
cargo install cargo-pgrx
cargo pgrx init

# Clone and build
git clone https://github.com/CrunchyData/pg_parquet
cd pg_parquet
cargo pgrx install --pg-config /path/to/pg_config

# In PostgreSQL
CREATE EXTENSION pg_parquet;
```

### pgpq (Python)

```bash
pip install pgpq pyarrow psycopg[binary]
```

### pg_duckdb

```bash
# Build from source (requires PostgreSQL dev headers)
git clone https://github.com/duckdb/pg_duckdb
cd pg_duckdb
make install

# In PostgreSQL
CREATE EXTENSION pg_duckdb;
```

### dlt (Python)

```bash
pip install "dlt[postgres]"
```

### Airbyte

```bash
# Docker Compose
git clone https://github.com/airbytehq/airbyte
cd airbyte
./run-ab-platform.sh

# Access at http://localhost:8000
```

---

## Appendix H: References and Resources

### Official Documentation

- **pgloader:** https://pgloader.io/
- **DuckDB postgres extension:** https://duckdb.org/docs/extensions/postgres
- **pg_duckdb:** https://github.com/duckdb/pg_duckdb
- **pg_parquet:** https://github.com/CrunchyData/pg_parquet
- **pgpq:** https://pypi.org/project/pgpq/
- **dlt:** https://dlthub.com/docs
- **Airbyte:** https://docs.airbyte.com/

### Performance Benchmarks

- **pg_duckdb benchmarks:** https://motherduck.com/product/postgres-integration/
- **DuckDB TPC-DS:** https://duckdb.org/docs/guides/performance/benchmarks
- **PostgreSQL COPY performance:** https://www.postgresql.org/docs/current/populate.html

### Community Resources

- **DuckDB Slack:** https://duckdb.org/slack
- **PostgreSQL mailing lists:** https://www.postgresql.org/list/
- **pgloader GitHub:** https://github.com/dimitri/pgloader

---

**End of Analysis**
